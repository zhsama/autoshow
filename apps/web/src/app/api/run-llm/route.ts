import { NextRequest, NextResponse } from 'next/server'

export async function POST(request: NextRequest) {
  try {
    const body = await request.json()
    const { 
      llmService,
      llmModel,
      prompt,
      transcript,
      frontMatter,
      metadata
    } = body
    
    // In a real application, you would call the actual LLM service
    // For now, we'll return mock generated content
    const mockLLMOutput = `# Show Notes

## Summary
This episode covers the key concepts of using AutoShow for automated show note generation.

## Key Topics
- Introduction to AutoShow
- Setting up transcription services
- Configuring LLM models
- Generating formatted show notes

## Timestamps
- 00:00 - Introduction
- 02:30 - Main topic discussion
- 10:15 - Q&A session
- 14:45 - Closing remarks

## Resources Mentioned
- AutoShow GitHub Repository
- Documentation website
- Community Discord

Generated by ${llmService} using model ${llmModel}`
    
    // Create a new show note with the generated content
    const newShowNote = {
      id: Date.now(),
      title: metadata?.title || 'Generated Show Notes',
      publishDate: new Date().toISOString().split('T')[0],
      llmOutput: mockLLMOutput,
      frontmatter: frontMatter,
      transcript: transcript,
      prompt: prompt
    }
    
    return NextResponse.json({ 
      showNote: newShowNote,
      llmCost: 0.02 // Mock cost in dollars
    })
  } catch (error) {
    console.error('[API] Error running LLM:', error)
    return NextResponse.json(
      { error: 'Failed to run LLM' },
      { status: 500 }
    )
  }
}